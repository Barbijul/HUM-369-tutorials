{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Création-du-compte-de-bot\" data-toc-modified-id=\"Création-du-compte-de-bot-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Création du compte de bot</a></span></li><li><span><a href=\"#Utilisation-de-pywikiapi\" data-toc-modified-id=\"Utilisation-de-pywikiapi-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Utilisation de pywikiapi</a></span><ul class=\"toc-item\"><li><span><a href=\"#Login\" data-toc-modified-id=\"Login-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Login</a></span></li><li><span><a href=\"#Lire-le-contenu-depuis-l'API\" data-toc-modified-id=\"Lire-le-contenu-depuis-l'API-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Lire le contenu depuis l'API</a></span></li><li><span><a href=\"#Éditer-wikipast-en-python\" data-toc-modified-id=\"Éditer-wikipast-en-python-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Éditer wikipast en python</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#Quelques-ressources-utiles-pour-Wikipast\" data-toc-modified-id=\"Quelques-ressources-utiles-pour-Wikipast-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quelques ressources utiles pour Wikipast</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Comme nous le verrons plus tard dans le cours, Wikipedia est peuplé par de nombreux bots qui sont les utilisateurs les plus prolifiques en terme de modifications brutes. Wikimedia, le logiciel qui fait tourner Wikipedia, possède donc une API très complète permettant de réaliser facilement la plupart des tâches basiques, à savoir retrouver des pages et les modifier.\n",
    "\n",
    "La documentation de cette API est trouvable en ligne à cette adresse: https://www.mediawiki.org/wiki/API:Main_page/fr. Cette API est utilisable à l'aide de requêtes web classiques, toutefois il existe plusieurs \"wrappers\" python permettant de l'utiliser plus facilement dans ce langage. Nous utiliserons dans ce tutoriel le wrapper [`pywikiapi`](https://github.com/nyurik/pywikiapi) qui est très léger et certainement suffisant pour notre cours, toutefois vous pouvez également jeter un coup d'oeil aux autres wrappers: https://www.mediawiki.org/wiki/API:Client_code.\n",
    "\n",
    "\n",
    "# Création du compte de bot\n",
    "Pour faire des appels de lectures à l'API, il n'est pas nécessaire d'avoir de compte, toutefois pour pouvoir modifier les pages, il est nécessaire de créer un compte de bot. Les étapes de créations sont les suivantes:\n",
    "\n",
    "1. se rendre sur la page http://wikipast.epfl.ch/wikipast/index.php/Sp%C3%A9cial:BotPasswords/,\n",
    "2. Créer un nouveau robot en choissant les bons droits de modifications en fonction de vos besoin (par exemple: \"Modification de gros volumes\", \"Modifier des pages existantes\", \"Créer, modifier et déplacer des pages\", \"Importer de nouveaux fichiers\", \"Téléverser, remplacer et renommer des fichiers\")\n",
    "3. La page suivante donne le mot de passe du bot, il faut bien le sauvegarder car il ne sera plus affiché, de plus il faut le garder secrer car il permet de modifier des pages sur Wikipast.\n",
    "\n",
    "\n",
    "Vous pouvez maintenant rentrer les identifiants du bots dans la cellule ci-dessous pour pouvoir l'utiliser dans ce tutoriel (et ne pas oublier de l'éxecuter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Username@yourbot'\n",
    "password = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation de pywikiapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywikiapi import Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pywikiapi` utilise un objet `Site` pour faire ses requête. Cet objet prend l'adresse de l'API et permet d'abstraire la plupart des appels à l'API derrière une syntaxe python simple. Seules deux fonctionallités de l'API sont implémentées spécifiquement dans `pywikiapi`: `login` et `query`, toutefois, tous les autres fonctionnalités de l'API peuvent être également invoquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = Site('http://wikipast.epfl.ch/wikipast/api.php') # Définition de l'adresse de l'API\n",
    "site.no_ssl = True # Désactivation du https, car pas activé sur wikipast\n",
    "site.login(user, password) # Login du bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant appelé la fonction [`login`](http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=login) de l'API.\n",
    "\n",
    "## Lire le contenu depuis l'API\n",
    "Les deux autres fonctions implémentées qui apellent explicitement l'API sont `site.query` et `site.query_pages`. En voici un exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# Cherche dans toutes les pages celles commençant par Leopold\n",
    "for res in site.query(list='allpages', apprefix='Mahatma'): \n",
    "    results.append(res)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons extraire de cette liste de résultats le nom des pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_names = []\n",
    "for res in results[0]['allpages']: # aller dans le dictionnaire nesté\n",
    "    pages_names.append(res['title'])\n",
    "pages_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois ces noms extraits, nous pouvons utiliser `site.query_pages` pour extraire certaines informations sur ces pages. La cellule suivante trouves les pages ayant un titre dans page_names et retourne les attributs \"categories\", \"links\", \"extlinks\" comme décrits dans la documentation: http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pages = []\n",
    "\n",
    "for res in site.query_pages(titles=page_names, prop=['categories', 'links', 'extlinks']): \n",
    "    results_pages.append(res)\n",
    "results_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutefois, ce qui va souvent nous intéresser est le contenu textuelle d'une page du wiki. `pywikiapi` n'implémente pas directement des fonctions pour faire cela, mais nous pouvons tout de même l'utiliser grâce à un appel direct à `Site`.\n",
    "\n",
    "En effet, en écrivant `site('parse', page='Mahatma Gandhi', prop=['wikitext'])`, `pywikiapi` fait directement un appel à la fonction `parse` de l'API, nous pouvons ensuite passer les autres arguments directement dans l'appel de la fonction et ils seront transmis à l'API. Pour plus de détails sur les arguments possibles, il faut de nouveau se référer à la documentation: http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=parse. Dans celle-ci, nous pouvons voir que `parse` prend un argument `page` qui indique la page à parser et un argument `prop` qui peut entre autre prendre la valeur `wikitext` et `sections` qui fait retourner à `parse` le contenu wikitext de la page ainsi qu'une liste de ses sections. En voici le résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = site('parse', page='Mahatma Gandhi', prop=['wikitext', 'sections'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant écrire une fonction qui prend un nom de page et nous retourne son wikitext et une autre qui nous retourne ses sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_text(page):\n",
    "    result = site('parse', page=page, prop=['wikitext'])\n",
    "    return result['parse']['wikitext']\n",
    "\n",
    "def get_sections(page):\n",
    "    result = site('parse', page=page, prop=['sections'])\n",
    "    return result['parse']['sections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant tester les fonctions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_wiki_text('Mahatma Gandhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sections('Mahatma Gandhi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Éditer wikipast en python\n",
    "\n",
    "Il nous reste donc à montrer comment éditer une page, l'API est défini sur cette page: http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=edit.\n",
    "\n",
    "Vous pouvez librement vous créer une page bac à sable où utiliser la page [bacasable](http://wikipast.epfl.ch/wikipast/index.php/Bacasable).\n",
    "\n",
    "Nous allons donc créer une nouvelle page et l'éditer depuis python, vous pouvez à chaque étape voir le résultat de vos modifications sur la page correspondante de wikipast.\n",
    "\n",
    "Commençons par créer la nouvelle page (modifiez la variable `titre`), soyez prudents, ce bout de code écrase entièrement la page (même s'il existait déjà):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titre = 'Bacasable - tutoriel'\n",
    "\n",
    "site('edit', title=titre,\n",
    "     text='Ceci est une nouvelle page.\\nAvec peu de contenu.',\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de noter que pour modifier une page (création, modification, supression), il faut absolument être connecté et avoir un token d'édition. Ce dernier peut être obtenu en appelant `site.token()` et devra donc être ajouter comme argument à chaque fonction éditant une page.\n",
    "\n",
    "Rajoutons maintenant deux nouvelles sections à notre page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "     section='new',\n",
    "     sectiontitle='Nouvelle section 1 de test',\n",
    "     text='Ceci est le texte de ma nouvelle section.',\n",
    "     token=site.token())\n",
    "site('edit', title=titre,\n",
    "     section='new',\n",
    "     sectiontitle='Nouvelle section 2 de test',\n",
    "     text='Ceci est le texte de ma nouvelle section.',\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant ajouter du texte à une de nos sections grâce aux arguments `prependtext` et `appendtext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "     section=2,\n",
    "     prependtext='Rajoutons du texte avant.\\n', # notez l'ajout d'un retour à la fin du texte ajouté\n",
    "     token=site.token())\n",
    "\n",
    "site('edit', title=titre,\n",
    "     section=2,\n",
    "     appendtext='\\nRajoutons du texte et après.\\n',\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le seul moyen d'être plus fin dans la modification de la page est de récupérer le wikitext de la page, de modifier sa string, puis d'écraser la page avec le texe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_wiki_text(titre)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('Nouvelle section', 'Sous-titre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "    text=text,\n",
    "    token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons donc vu comment récuperer des informations des pages de wikipast et comment éditer les pages. Toutefois, beaucoup d'options de l'API n'ont pas été couvertes ici, il est donc judicieux de s'y référer lorsque que vous créerez vos bots afin de ne pas manquer une fonctionalité qui pourrait déjà être implémentée avant de la réinventer vous-même."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques ressources utiles pour Wikipast\n",
    "\n",
    "Afin de pouvoir traiter les ~670'000 pages présentes sur Wikipast, quelques optimisations sont nécessaire.\n",
    "\n",
    "Voici donc quelques bouts de codes permettant de parcourir toutes les pages efficacements et deux fichiers json précalculés qui vous permettront de ne pas avoir à la refaire trop souvent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # Libraire utile pour voir le progrès d'une boucle\n",
    "import multiprocessing as mp # Librairie d'execution parallèle\n",
    "import gzip # Permet d'économiser beaucoup de place en compressant les fichiers json\n",
    "import json\n",
    "\n",
    "# Read a gzipped json file\n",
    "def load_gzip_json(path):\n",
    "    with gzip.GzipFile(path, 'r') as infile:\n",
    "        return json.loads(infile.read().decode('utf-8'))\n",
    "\n",
    "def write_gzip_json(data, path):\n",
    "    with gzip.GzipFile(path, 'w') as outfile:\n",
    "        outfile.write(json.dumps(data).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier bout de code permet de récuper les titres et id de toutes les pages du wiki, il prend ~1h à être éxécuté. La version actuelle a été compilée le 15.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8693fb90794b4a5ab5e0c574fb5d42b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# all_pages = []\n",
    "# for r in tqdm(site.query(list='allpages')):\n",
    "#     for page in r['allpages']:\n",
    "#         all_pages.append(page)\n",
    "# write_gzip_json(all_pages, 'all_pages.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pageid': 228702, 'ns': 0, 'title': 'Cecil Georges-Bazile'},\n",
       " {'pageid': 281912, 'ns': 0, 'title': 'Cecil Grayson'},\n",
       " {'pageid': 239845, 'ns': 0, 'title': 'Cecil H. Brown'},\n",
       " {'pageid': 241900, 'ns': 0, 'title': 'Cecil H. Uyehara'},\n",
       " {'pageid': 539771, 'ns': 0, 'title': 'Cecil Herbert Stuart Fifoot'},\n",
       " {'pageid': 379430, 'ns': 0, 'title': 'Cecil Hill'},\n",
       " {'pageid': 179638, 'ns': 0, 'title': 'Cecil James Sharp'},\n",
       " {'pageid': 254418, 'ns': 0, 'title': 'Cecil Jane'},\n",
       " {'pageid': 462835, 'ns': 0, 'title': 'Cecil Jermyn Brown'},\n",
       " {'pageid': 386899, 'ns': 0, 'title': 'Cecil John Layton Price'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pages = load_gzip_json('./all_pages.json.gz')\n",
    "all_pages[100010:100020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce deuxième bout de code récupère la plupart des infos intéressante que l'on pourrait vouloir d'une page. Il lance le processus en parralèle sur 16 threads, rendant le tout beaucoup plus rapide. Le pattern de code de parallèlisation peut-être réutiliser pour vos propre bots qui devraient traiter beaucoup de pages.\n",
    "\n",
    "Attention, ce code qui traite autant de page est très lourd pour nos serveurs et est long à éxecuter. La version actuelle a été compilée le 15.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 676372 out of 676372.\n"
     ]
    }
   ],
   "source": [
    "# def get_all_page_infos(page):\n",
    "#     return site('parse', page=page, prop=['wikitext','links',\n",
    "#                                           'externallinks', 'categories',\n",
    "#                                           'revid', 'images', 'sections'])\n",
    "\n",
    "# pages_titles = [page['title'] for page in all_pages] # la syntaxe est une list comprehension, c.f. https://docs.python.org/fr/3/tutorial/datastructures.html#list-comprehensions\n",
    "\n",
    "# # Création d'une pool avec 16 process\n",
    "# pool = mp.Pool(16)\n",
    "# # Création d'une barre de chargement et d'une fonction pour la mettre à jour.\n",
    "# pbar = tqdm(total=len(pages_titles))\n",
    "# def update(*a):\n",
    "#     pbar.update()\n",
    "\n",
    "# # Récupération de tous les résultats en parallèle\n",
    "# results = []\n",
    "# for page in pages_titles:\n",
    "#     results.append(pool.apply_async(get_all_page_infos, args=(page,), callback=update))\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# # Certaines requête ont inévitablement raté, il faut donc récupérer le reste\n",
    "# all_results = []\n",
    "# num_failures = 0\n",
    "# for idx, result in enumerate(results):\n",
    "#     try:\n",
    "#         all_results.append(result.get()['parse'])\n",
    "#     except:\n",
    "#         num_failures += 1\n",
    "# print(f\"There is {num_failures} that failed.\")\n",
    "\n",
    "# # Trouvons maintenant les titre des pages qui ont été récupérées\n",
    "# valid_pages_titles = set([result['title'] for result in all_results])\n",
    "    \n",
    "# # Trouvons maintenant les pages qui n'ont pas été récupérées\n",
    "# missing_pages_titles = set(pages_titles).difference(valid_pages_titles)\n",
    "\n",
    "# # Et récuperons les\n",
    "# for page in missing_pages_titles:\n",
    "#     all_results.append(get_all_page_infos(page))\n",
    "    \n",
    "# print(f\"We now have {len(all_results)} out of {len(all_pages)}.\")\n",
    "\n",
    "\n",
    "# # Finalement, écrivons le fichier avec les résultats\n",
    "# write_gzip_json(all_results, 'all_data.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cecil Hill',\n",
       "  'pageid': 379430,\n",
       "  'revid': 1033163,\n",
       "  'categories': [],\n",
       "  'links': [],\n",
       "  'images': [],\n",
       "  'externallinks': ['https://www.wikidata.org/wiki/Q49018849',\n",
       "   'https://www.wikidata.org/wiki/Q5',\n",
       "   'http://data.bnf.fr/ark:/12148/cb129571151'],\n",
       "  'sections': [],\n",
       "  'wikitext': \"Wikidata: [https://www.wikidata.org/wiki/Q49018849 Q49018849] ([https://www.wikidata.org/wiki/Q5 Q5]) ''Uncertain identification''\\n\\nBnF ID: [http://data.bnf.fr/ark:/12148/cb129571151 129571151]\"},\n",
       " {'title': 'Cecil James Sharp',\n",
       "  'pageid': 179638,\n",
       "  'revid': 857175,\n",
       "  'categories': [],\n",
       "  'links': [{'ns': 0, 'title': 'Décès', 'exists': True},\n",
       "   {'ns': 0, 'title': 'Londres', 'exists': True},\n",
       "   {'ns': 0, 'title': 'Naissance', 'exists': True},\n",
       "   {'ns': 0, 'title': '1859.11.22', 'exists': False},\n",
       "   {'ns': 0, 'title': '1924.06.28', 'exists': False}],\n",
       "  'images': [],\n",
       "  'externallinks': ['https://www.wikidata.org/wiki/Q3889019',\n",
       "   'https://www.wikidata.org/wiki/Q5',\n",
       "   'http://data.bnf.fr/ark:/12148/cb14820854n'],\n",
       "  'sections': [],\n",
       "  'wikitext': 'Wikidata: [https://www.wikidata.org/wiki/Q3889019 Q3889019] ([https://www.wikidata.org/wiki/Q5 Q5])\\n\\nBnF ID: [http://data.bnf.fr/ark:/12148/cb14820854n 14820854n]\\n\\n*[[1859.11.22]] / [[Londres]]. [[Naissance]] de [[Cecil James Sharp]]. [http://data.bnf.fr/ark:/12148/cb14820854n]\\n\\n*[[1924.06.28]] / [[Londres]]. [[Décès]] de [[Cecil James Sharp]]. [http://data.bnf.fr/ark:/12148/cb14820854n]'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = load_gzip_json('./all_data.json.gz')\n",
    "all_data[100000:100002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces deux fichiers json peuvent donc être utiles pour explorer les données sans avoir à faire des querys sur toute la base de donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, écrivons deux query qui peuvent être utiles comme points de départ pour un bot. \n",
    "\n",
    "La première qui nous permets de récuper tous les liens présents sur la page de biographies et qui peut être utile comme point de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ns': 0, 'title': 'Albert Cohen', 'exists': True},\n",
       " {'ns': 0, 'title': 'Alberto Giacometti', 'exists': True},\n",
       " {'ns': 0, 'title': 'Alfred Cortot', 'exists': True},\n",
       " {'ns': 0, 'title': 'André Breton', 'exists': True},\n",
       " {'ns': 0, 'title': 'Audrey Hepburn', 'exists': True},\n",
       " {'ns': 0, 'title': 'Ayrton Senna', 'exists': True},\n",
       " {'ns': 0, 'title': 'Bio', 'exists': True},\n",
       " {'ns': 0, 'title': 'Bjorn Borg', 'exists': True},\n",
       " {'ns': 0, 'title': 'Charles Aznavour', 'exists': True},\n",
       " {'ns': 0, 'title': 'Charlie Chaplin', 'exists': True},\n",
       " {'ns': 0, 'title': 'Claude Monet', 'exists': True},\n",
       " {'ns': 0, 'title': 'Constantin Regamey', 'exists': True},\n",
       " {'ns': 0, 'title': 'David Bowie', 'exists': True},\n",
       " {'ns': 0, 'title': 'Emile Zola', 'exists': True},\n",
       " {'ns': 0, 'title': 'Enzo Ferrari', 'exists': True},\n",
       " {'ns': 0, 'title': 'Ferdinand Hodler', 'exists': True},\n",
       " {'ns': 0, 'title': 'François Perréard', 'exists': True},\n",
       " {'ns': 0, 'title': 'Friedrich Dürrenmatt', 'exists': True},\n",
       " {'ns': 0, 'title': 'Gabrielle Antille', 'exists': True},\n",
       " {'ns': 0, 'title': 'Gustave Altherr', 'exists': True},\n",
       " {'ns': 0, 'title': 'Hans Ruedi Giger', 'exists': True},\n",
       " {'ns': 0, 'title': 'Hermann Hesse', 'exists': True},\n",
       " {'ns': 0, 'title': 'Jacques Chirac', 'exists': True},\n",
       " {'ns': 0, 'title': 'Jean-Luc Godard', 'exists': True},\n",
       " {'ns': 0, 'title': 'Jean-Paul Sartre', 'exists': True},\n",
       " {'ns': 0, 'title': 'Jean-Pierre Bregnard', 'exists': True},\n",
       " {'ns': 0, 'title': 'Jean Calvin', 'exists': True},\n",
       " {'ns': 0, 'title': 'Jeanne Hersch', 'exists': True},\n",
       " {'ns': 0, 'title': 'John Fitzgerald Kennedy', 'exists': True},\n",
       " {'ns': 0, 'title': 'John Lennon', 'exists': True},\n",
       " {'ns': 0, 'title': 'Juan Manuel Fangio', 'exists': True},\n",
       " {'ns': 0, 'title': 'Julien Perrot', 'exists': True},\n",
       " {'ns': 0, 'title': 'Kurt Cobain', 'exists': True},\n",
       " {'ns': 0, 'title': 'Louis De Funès', 'exists': True},\n",
       " {'ns': 0, 'title': 'Louis Lumière', 'exists': True},\n",
       " {'ns': 0, 'title': 'Louise Michel', 'exists': True},\n",
       " {'ns': 0, 'title': 'Mahatma Gandhi', 'exists': True},\n",
       " {'ns': 0, 'title': 'Marcel Jufer', 'exists': True},\n",
       " {'ns': 0, 'title': 'Marguerite Duras', 'exists': True},\n",
       " {'ns': 0, 'title': 'Marguerite Yourcenar', 'exists': True},\n",
       " {'ns': 0, 'title': 'Mario Botta', 'exists': True},\n",
       " {'ns': 0, 'title': 'Markus Kamber', 'exists': True},\n",
       " {'ns': 0, 'title': 'Maurice Cosandey', 'exists': True},\n",
       " {'ns': 0, 'title': 'Max Planck', 'exists': True},\n",
       " {'ns': 0, 'title': 'Michael Jackson', 'exists': True},\n",
       " {'ns': 0, 'title': 'Nicolas II', 'exists': True},\n",
       " {'ns': 0, 'title': 'Otto von Bismarck', 'exists': True},\n",
       " {'ns': 0, 'title': 'Pablo Picasso', 'exists': True},\n",
       " {'ns': 0, 'title': 'Patrice Borcard', 'exists': True},\n",
       " {'ns': 0, 'title': 'Patrice Haesslein', 'exists': True},\n",
       " {'ns': 0, 'title': 'Philippe Jaccottet', 'exists': True},\n",
       " {'ns': 0, 'title': 'Pierre de Coubertin', 'exists': True},\n",
       " {'ns': 0, 'title': 'Rod Laver', 'exists': True},\n",
       " {'ns': 0, 'title': 'Roger Monney', 'exists': True},\n",
       " {'ns': 0, 'title': 'Salvador Dalí', 'exists': True},\n",
       " {'ns': 0, 'title': 'Sam Humbert', 'exists': True},\n",
       " {'ns': 0, 'title': 'Serge Gainsbourg', 'exists': True},\n",
       " {'ns': 0, 'title': 'Simone de Beauvoir', 'exists': True},\n",
       " {'ns': 0, 'title': 'Stanley Kubrick', 'exists': True},\n",
       " {'ns': 0, 'title': 'Victor Hugo', 'exists': True},\n",
       " {'ns': 0, 'title': 'William Grenier', 'exists': True},\n",
       " {'ns': 0, 'title': 'Wolfgang Amadeus Mozart', 'exists': True},\n",
       " {'ns': 0, 'title': 'Yuri Gagarin', 'exists': True}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site('parse', page='Biographies', prop='links')['parse']['links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième utilise le fait que certains bots ont ajouté un lien externe vers l'attrbut [Q5](https://www.wikidata.org/wiki/Q5) de wikidata qui indique que la page parle d'un humain, cela peut donc être utilisé pour récupérer toutes les pages parlants d'humains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifie si https://www.wikidata.org/wiki/Q5 est dans la liste de liens externes\n",
    "# Utilise une liste comprehension avec un filtre conditionnel\n",
    "humans = [data['title'] for data in all_data if \n",
    "          'externallinks' in data and\n",
    "          'https://www.wikidata.org/wiki/Q5' in data['externallinks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 497586 pages about humans.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Alan Henry',\n",
       " 'Alan Henry Linton',\n",
       " 'Alan Herdman',\n",
       " 'Alan Heuser',\n",
       " 'Alan Hewitt',\n",
       " 'Alan Hills',\n",
       " 'Alan Hindley',\n",
       " 'Alan Hirshfeld',\n",
       " 'Alan Hodson',\n",
       " 'Alan Holden']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"There is {len(humans)} pages about humans.\")\n",
    "humans[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une dernière note, si vous faites des modifications massives. Soyez prudents en vérifiant avant chaque modification que le `revid` de la page que vous avez récuperez avant modification est le même que celui de la page qui va être modifiée. C'est à dire, si vous précalculez toutes les pages à être modifiées, récuperez également leurs `revid`. Ensuite, au moment de la modification, vérifier que le `revid` de la page qui est en ligne est le même que le votre, sinon vous allez écrasez une nouvelle modification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
